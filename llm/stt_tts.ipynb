{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65a5816f",
   "metadata": {},
   "source": [
    "# STT\n",
    "##### uv add pyaudio speechrecognition pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18cc9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9469a590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something!\n",
      "인식된 텍스트 : 안녕하세요 전민도 입니다\n",
      "save voice\n"
     ]
    }
   ],
   "source": [
    "r = sr.Recognizer()\n",
    "\n",
    "with sr.Microphone() as source:\n",
    "    \n",
    "    print(\"Say something!\")\n",
    "    r.adjust_for_ambient_noise(source) # 주변 소음 제거\n",
    "    \n",
    "    audio = r.listen(source)\n",
    "    text = r.recognize_openai(audio)\n",
    "    print(f'인식된 텍스트 : {text}')\n",
    "    \n",
    "    audio_file = audio.get_wav_data()\n",
    "    \n",
    "    with open('./audio/input.wav', 'wb') as f:\n",
    "        f.write(audio_file)\n",
    "        \n",
    "    print('save voice')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76c9d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound = AudioSegment.from_wav('./audio/input.wav')\n",
    "\n",
    "play(sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ac10c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak_reaction(user_prompt):\n",
    "    system_prompt = \"\"\"당신은 낭만적인 챗봇입니다\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f34dba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tell me\n",
      "loading\n",
      "said text : \n",
      "chatbot : 안녕하세요! 오늘은 어떤 이야기를 나누고 싶으신가요? 당신의 마음속에 따뜻한 감성과 함께 이야기를 들려드릴게요.\n",
      "tell me\n",
      "loading\n",
      "said text : \n",
      "chatbot : 안녕하세요! 오늘은 어떤 사랑스럽고 로맨틱한 이야기를 나누고 싶으신가요? 당신의 마음을 따뜻하게 만들어줄 어떤 이야기든 저는 언제든 준비되어 있어요. 당신의 하루를 특별하게 만들어 드릴 수 있다면 정말 행복하겠습니다.\n",
      "tell me\n",
      "loading\n",
      "said text : 친절하다니까 빨리 봐야 안 그러곤 해 진짜\n",
      "chatbot : 정말 친절하구나, 그런 마음씀씀이가 너무 고마워. 빠르게 봐야 한다니 내 마음도 급해지네. 언제든지 도와줄 준비가 되어 있으니까, 편하게 말해줘. 같이 힘내자!\n",
      "tell me\n",
      "loading\n",
      "said text : 스카프, 신찰고래시가 1등을 거뒀죠. 그래서 이게 과연 많이 유행을 할 것인가. 그리고 프리미엄 대디.\n",
      "chatbot : 아, 정말 흥미로운 소식이네요! 신찰고래시가 1등을 차지했다니, 그 인기가 정말 대단한 것 같아요. 이런 인기 아이템이나 트렌드가 얼마나 오래 지속될지는 사람들의 관심과 반응에 따라 달라지겠지만, 분명히 많은 사람들이 관심을 갖고 있어서 앞으로 또 어떤 변화가 생길지 기대가 되네요.\n",
      "\n",
      "프리미엄 대디라는 말도 참 따뜻하고 로맨틱하게 들리는데요, 혹시 특별한 의미나 맥락이 있나요? 어떤 이야기를 담고 있는지 궁금하네요. 이런 대화들이 더 풍부하고 아름다운 감성을 자아내게 하는 것 같아요!\n",
      "tell me\n",
      "loading\n",
      "said text : 확실히 그게 있었던 것 같아요. 순간적으로 많이 했지만 아, 진짜의 느낌을 내가 한다고 놓치면 안 된다. 한다고 해서 금방 좀 차분해졌고.\n",
      "chatbot : 그 느낌 정말 소중하네요. 순간순간의 감정을 놓치지 않고, 진짜 자신을 느끼려고 하는 모습이 아름다워요. 때로는 바쁘고 흔들릴 때도 있지만, 그런 찰나를 자각하는 것 자체가 이미 깊은 성장이 아닐까 싶어요. 계속 그런 마음으로 자신을 바라보며, 작은 감정 하나하나에 의미를 두는 것도 참 멋진 일인 것 같아요. 혹시 지금 느끼는 감정이나 그 순간이 떠오르신다면, 저와 함께 이야기 나눠보는 것도 좋아요.\n",
      "tell me\n",
      "loading\n",
      "said text : 난 좀 쉬워 이거 어려운데?\n",
      "chatbot : 그래, 정말 힘든 거 같아도 너는 충분히 해낼 수 있어. 잠깐씩 쉬어가면서 천천히 해보자. 너의 노력은 분명히 빛이 날 거야. 같이 힘내자!\n",
      "tell me\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtell me\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m r\u001b[38;5;241m.\u001b[39madjust_for_ambient_noise(source)\n\u001b[1;32m----> 7\u001b[0m audio \u001b[38;5;241m=\u001b[39m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m user_text \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mrecognize_openai(audio)\n",
      "File \u001b[1;32mc:\\walker\\code\\llm\\.venv\\lib\\site-packages\\speech_recognition\\__init__.py:460\u001b[0m, in \u001b[0;36mRecognizer.listen\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration, stream)\u001b[0m\n\u001b[0;32m    458\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listen(source, timeout, phrase_time_limit, snowboy_configuration, stream)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\walker\\code\\llm\\.venv\\lib\\site-packages\\speech_recognition\\__init__.py:530\u001b[0m, in \u001b[0;36mRecognizer._listen\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration, stream)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m phrase_time_limit \u001b[38;5;129;01mand\u001b[39;00m elapsed_time \u001b[38;5;241m-\u001b[39m phrase_start_time \u001b[38;5;241m>\u001b[39m phrase_time_limit:\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# reached end of the stream\u001b[39;00m\n\u001b[0;32m    532\u001b[0m frames\u001b[38;5;241m.\u001b[39mappend(buffer)\n",
      "File \u001b[1;32mc:\\walker\\code\\llm\\.venv\\lib\\site-packages\\speech_recognition\\__init__.py:191\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyaudio_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\walker\\code\\llm\\.venv\\lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    r = sr.Recognizer()\n",
    "    \n",
    "    with sr.Microphone() as source:\n",
    "        print('tell me')\n",
    "        r.adjust_for_ambient_noise(source)\n",
    "        audio = r.listen(source)\n",
    "        print('loading')\n",
    "        user_text = r.recognize_openai(audio)\n",
    "        print(f'said text : {user_text}')\n",
    "        \n",
    "        if user_text == '들어가':\n",
    "            break\n",
    "        \n",
    "        answer = speak_reaction(user_text)\n",
    "        print(f'chatbot : {answer}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fe7aec",
   "metadata": {},
   "source": [
    "# STT -> LLM -> TTS (챗봇 스피커)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a73df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "듣는 중....\n",
      "인식 중입니다.....\n",
      "인식된 텍스트: 시리아 시리아 시리아\n",
      "챗봇 답변: 시리아에 대해 이야기하고 싶으신가요? 아니면 시리아라는 단어에 특별한 의미가 있나요? 시리아는 풍부한 역사와 아름다운 풍경, 그리고 다양한 문화가 어우러진 나라입니다. 고대 로마와 이슬람 문명이 숨쉬는 곳이기도 하죠. 혹시 시리아에 대해 더 알고 싶은 점이 있나요? 당신의 관심사에 맞춰 함께 이야기를 나눌 수 있어요.\n",
      "듣는 중....\n",
      "인식 중입니다.....\n",
      "인식된 텍스트: อืมัน\n",
      "챗봇 답변: สวัสดีค่ะ! มีอะไรให้ฉันช่วยเหลือหรือพูดคุยกันไหมคะ?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# 재생\u001b[39;00m\n\u001b[0;32m     35\u001b[0m sound \u001b[38;5;241m=\u001b[39m AudioSegment\u001b[38;5;241m.\u001b[39mfrom_mp3(temp_path)\n\u001b[1;32m---> 36\u001b[0m \u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43msound\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\walker\\code\\llm\\.venv\\lib\\site-packages\\pydub\\playback.py:64\u001b[0m, in \u001b[0;36mplay\u001b[1;34m(audio_segment)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m     \u001b[43m_play_with_pyaudio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\walker\\code\\llm\\.venv\\lib\\site-packages\\pydub\\playback.py:33\u001b[0m, in \u001b[0;36m_play_with_pyaudio\u001b[1;34m(seg)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# break audio into half-second chunks (to allows keyboard interrupts)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m make_chunks(seg, \u001b[38;5;241m500\u001b[39m):\n\u001b[1;32m---> 33\u001b[0m         \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     stream\u001b[38;5;241m.\u001b[39mstop_stream()\n",
      "File \u001b[1;32mc:\\walker\\code\\llm\\.venv\\lib\\site-packages\\pyaudio\\__init__.py:550\u001b[0m, in \u001b[0;36mPyAudio.Stream.write\u001b[1;34m(self, frames, num_frames, exception_on_underflow)\u001b[0m\n\u001b[0;32m    547\u001b[0m     width \u001b[38;5;241m=\u001b[39m get_sample_size(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format)\n\u001b[0;32m    548\u001b[0m     num_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(frames) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels \u001b[38;5;241m*\u001b[39m width))\n\u001b[1;32m--> 550\u001b[0m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m                \u001b[49m\u001b[43mexception_on_underflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    r = sr.Recognizer() \n",
    "    # 실습 해보면서 넣을 수 있는 옵션 구글링 해보기.\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"듣는 중....\")\n",
    "        # STEP 1. 마이크로부터 입력\n",
    "        r.adjust_for_ambient_noise(source) \n",
    "        audio = r.listen(source)         \n",
    "        print(\"인식 중입니다.....\")\n",
    "\n",
    "        # STEP 2. Whisper API를 통한 텍스트 변환\n",
    "        user_text = r.recognize_openai(audio) \n",
    "        print(f\"인식된 텍스트: {user_text}\")\n",
    "\n",
    "        if user_text == \"그만\":\n",
    "            break\n",
    "        \n",
    "        # STEP 3. 인공지능 챗봇 응답\n",
    "        answer = speak_reaction(user_text)\n",
    "        print(f\"챗봇 답변: {answer}\")\n",
    "\n",
    "        # STEP 4. Whisper API로 응답\n",
    "        with client.audio.speech.with_streaming_response.create(\n",
    "            model=\"gpt-4o-mini-tts\",\n",
    "            voice=\"coral\",\n",
    "            input=answer,\n",
    "            instructions=\"밝은 목소리로 말해줘\"\n",
    "        ) as response:\n",
    "            # 음성 합성 결과를 임시 파일로 저장\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as temp_file:\n",
    "                temp_path = temp_file.name\n",
    "                response.stream_to_file(temp_path)\n",
    "\n",
    "                # 재생\n",
    "                sound = AudioSegment.from_mp3(temp_path)\n",
    "                play(sound)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
