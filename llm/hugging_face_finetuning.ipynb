{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c331ed",
   "metadata": {},
   "source": [
    "# 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed7ffc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수 불러오기\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "login(token=os.getenv(\"HUGGINGFACE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d792c853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n",
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.__version__)\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f135d241",
   "metadata": {},
   "source": [
    "# 기존 사전학습 모델 체험하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2234d1",
   "metadata": {},
   "source": [
    "## 1. 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f97e44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# 8비트(LoRA + int8) —— 정확도 보존↑, 메모리 절약은 중간(≈2~3배)\n",
    "## VRAM 12GB에서 안정/보수적으로 돌리고 싶을 때 기본 선택\n",
    "## 추론 품질 손실을 최소화하고 싶을 때도 선호\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,          # ✅ 가중치를 8bit로 로드(메모리 절감, 정확도 손실 적음)\n",
    "    load_in_4bit=False,         # 4bit 경로 비활성화(헷갈림 방지)\n",
    "\n",
    "    # llm_int8_threshold:\n",
    "    #  - outlier(이상치) 채널을 감지해 해당 채널만 고정밀 경로로 우회하는 기준.\n",
    "    #  - 기본 6.0이 널리 사용. 숫자를 낮추면 우회 채널이 늘어 정밀↑(메모리/속도↓).\n",
    "    llm_int8_threshold=6.0,\n",
    "\n",
    "    # llm_int8_has_fp16_weight:\n",
    "    #  - 일부 레이어(예: outlier가 많은 레이어)의 가중치를 fp16로 유지할지 여부.\n",
    "    #  - False면 전반적으로 8bit 경로에 머무름(메모리↓). True면 더 보수적(정확도↑, 메모리↑).\n",
    "    llm_int8_has_fp16_weight=False,\n",
    ")\n",
    "\n",
    "# 4비트(QLoRA) —— 메모리 절약 극대화(≈4~5배), 대형 모델/대량 배치에 유리\n",
    "## 4070 12GB에서 최대한 큰 모델/배치를 돌리고 싶을 때 선택\n",
    "## 약간의 정밀도 손실 감수 가능할 때\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,          # 가중치를 4bit로 로드(QLoRA 핵심, VRAM 절약 극대화)\n",
    "#     load_in_8bit=False,         # 8bit 경로 비활성화\n",
    "\n",
    "#     # 연산 정밀도(계산 dtype):\n",
    "#     # - 가중치는 4bit로 ‘보관’하지만, 실제 곱셈/합산은 16비트로 수행.\n",
    "#     # - 4070(아다러너)라면 bfloat16 권장(underflow에 강하고 안정적).\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,  # 또는 torch.float16 (환경에 따라 테스트)\n",
    "\n",
    "#     # Double Quantization:\n",
    "#     # - 1차 4bit 양자화에 쓰인 스케일/제로포인트 자체를 다시 압축.\n",
    "#     # - 메모리 추가 절감 효과. 보통 품질 영향은 미미해 ON 권장.\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "\n",
    "#     # 4bit 포맷:\n",
    "#     # - \"nf4\": NormalFloat4. LLM 분포에 맞춘 4bit 포맷으로 QLoRA 기본 추천.\n",
    "#     # - \"fp4\": Float4. 연구/특수 목적이 아니라면 nf4가 일반적으로 더 좋음.\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c9dabe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2882c66e5fac48889d4993c1d11e0a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"google/gemma-2-2b-it\" # \"google/gemma-2b-it\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    quantization_config=quantization_config,\n",
    "    dtype=torch.bfloat16,   # torch_dtype -> dtype\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "# pip install huggingface_hub[hf_xet] 다운로드 속도 높아짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1c7d8",
   "metadata": {},
   "source": [
    "## 2. 토크나이저 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c892f89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <eos>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(tokenizer.pad_token, tokenizer.eos_token)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "# [Tip] 파인튜닝 시 패딩을 안전하기 위한 설정\n",
    "# 1. tokenizer.pad_token = tokenizer.eos_token\n",
    "# 패딩 토큰이 되어 있지 않은 경우가 있어 만약 패딩이 필요할 때 오류가 날 수 있음(pad_token=None)\n",
    "# 2. tokenizer.padding_side = \"right\"\n",
    "# 모델이 왼쪽에서 오른쪽으로 단어를 예측하기 때문에, 오른쪽으로 설정하는게 좋음\n",
    "# 이 설정을 사용하면, 예를 들어 최대 길이가 8인 모델에 \"안녕하세요\"라는 문장을 입력할 때, 토크나이저는 다음과 같이 처리합니다.\n",
    "# 원래 토큰 시퀀스: [안, 녕, 하, 세, 요]\n",
    "# 패딩된 토큰 시퀀스: [안, 녕, 하, 세, 요, <pad>, <pad>, <pad>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aebbb39",
   "metadata": {},
   "source": [
    "### 1) 토크나이저 파라미터 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fa841b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000000000000019884624838656\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dfadc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80bdf0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 238179, 243415, 204551, 1], [2, 238559, 236361, 239779, 237526], [2, 238179, 243415, 239055, 74715]], 'attention_mask': [[1, 1, 1, 1, 0], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 패딩 테스트 \n",
    "# 토큰화는 글자 단위로 쪼개는게 아니라 의미 단위로 쪼개는 것 주의\n",
    "test = tokenizer(\n",
    "    [\"안녕하세요\", \"반가워요\", \"안녕히 계세요\"],\n",
    "    max_length=5,            # 최대 길이를 5로 고정\n",
    "    truncation=True,         # 길이가 5보다 크면 잘라냄\n",
    "    padding=\"max_length\",    # 길이가 5보다 짧으면 패딩처리\n",
    "    padding_side=\"right\"     # 패딩은 오른쪽에 붙임\n",
    ")\n",
    "\n",
    "test\n",
    "# {'input_ids': [[2, 238179, 243415, 204551, 1], [2, 238559, 236361, 239779, 237526], [2, 238179, 243415, 239055, 74715]]\n",
    "# 반복되는 2는 <bos> / 2381799 243415는 안녕"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae366d61",
   "metadata": {},
   "source": [
    "### 2) 토크나이저 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e025fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장의 경계를 알려주는 토큰: <bos>, <eos>\n",
    "# <bos> (begin of sequence) - 문장 시작\n",
    "# <eos> (end of sequence) - 문장 끝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78c972aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 238179, 243415, 204551], 'attention_mask': [1, 1, 1, 1]}\n",
      "<bos>안녕하세요\n"
     ]
    }
   ],
   "source": [
    "text = \"안녕하세요\"\n",
    "\n",
    "# 텍스트를 숫자로 토크나이징\n",
    "tokenized_text = tokenizer(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "# 숫자를 텍스트로 디코딩\n",
    "decoded_text = tokenizer.decode(tokenized_text[\"input_ids\"], skip_special_tokens=False)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aeced2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2,    106,   1645,    108, 240471, 240046, 101969, 235248, 241305,\n",
      "         239042, 237138, 237014,  96673,    108,  70685, 243415, 204551,    107,\n",
      "            108,    106,   2516,    108, 238179, 243415, 204551, 235265,  78821,\n",
      "         236361, 239779, 237526,    107,    108]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['<bos><start_of_turn>user\\n친절하게 답변해주세요\\n 안녕하세요<end_of_turn>\\n<start_of_turn>model\\n안녕하세요. 반가워요<end_of_turn>\\n']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<bos><start_of_turn>user\n",
      "친절하게 답변해주세요\n",
      " 안녕하세요<end_of_turn>\n",
      "<start_of_turn>model\n",
      "안녕하세요. 반가워요<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 대부분의 it 모델들은 system이라는 role을 이해하지 못해 user에 system 정보 추가하는 것으로 추가\n",
    "# TemplateError: System role not supported\n",
    "#  {\n",
    "#       \"role\": \"system\",\n",
    "#       \"content\": \"너는 시스템이야\"\n",
    "#   },\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"친절하게 답변해주세요\\n 안녕하세요\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"안녕하세요. 반가워요\"\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "# 대화 구조를 숫자로 토크나이징\n",
    "tokenized_messages = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=False,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(tokenized_messages)\n",
    "\n",
    "print('-' * 100)\n",
    "\n",
    "tokenized_messages_str = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=False,\n",
    "    tokenize=False,\n",
    ")\n",
    "print(tokenized_messages_str)\n",
    "\n",
    "print('-' * 100)\n",
    "\n",
    "# 숫자를 텍스트로 디코딩\n",
    "decoded_messages = tokenizer.decode(\n",
    "    tokenized_messages[\"input_ids\"][0],\n",
    "    skip_special_tokens=False\n",
    ")\n",
    "print(decoded_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2df30c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'친'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([240471])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2f77d",
   "metadata": {},
   "source": [
    "# 학습 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b94151",
   "metadata": {},
   "source": [
    "## 1. 학습 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8168e0",
   "metadata": {},
   "source": [
    "### 1) HuggingFace Dataset 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25ae8234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f190594a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 22194\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 2466\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 2740\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2959bc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset = dataset['train'].select(range(10))\n",
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89d67cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts(example):\n",
    "    messages = [\n",
    "        {\"role\": \"user\",\n",
    "            \"content\": \"다음 글을 요약해주세요:\\n\\n {}\".format(example['document'])}, \n",
    "        {\"role\": \"assistant\",\n",
    "            \"content\": \"{}\".format(example['summary'])}\n",
    "    ]\n",
    "    chat_message = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    example[\"text\"] = chat_message+\"<eos>\"\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12c7616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = sample_dataset.map(generate_prompts) # map(빠름!)으로 각 요소에 함수를 적용(keywords라는 key 추가됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ace62a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '2022-07-03 17:14:37',\n",
       " 'category': 'economy',\n",
       " 'press': 'YTN ',\n",
       " 'title': '추경호 중기 수출지원 총력 무역금융 40조 확대',\n",
       " 'document': '앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.',\n",
       " 'link': 'https://n.news.naver.com/mnews/article/052/0001759333?sid=101',\n",
       " 'summary': '올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.',\n",
       " 'text': '<bos><start_of_turn>user\\n다음 글을 요약해주세요:\\n\\n 앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.<end_of_turn>\\n<start_of_turn>model\\n올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.<end_of_turn>\\n<eos>'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf007dc",
   "metadata": {},
   "source": [
    "### 2) 커스텀 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "874125ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [목표] 다음 신문을 요약해주세요. 그리고 핵심 키워드 5개를 뽑아주세요.\n",
    "\n",
    "# it 모델을 파인튜닝하는 이유 - 응답 결과의 일관성을 만들기 위해서(지시 사항을 함께 학습 시키기 위함)\n",
    "# ex) 신문 기사를 요약하고, 핵심 키워드 5개를 뽑아라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ffcfeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP1. 위에서 불러온 데이터를 가지고 keywords를 뽑아주는 데이터를 만들어야 한다.\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "# {'date': '2022-07-03 17:14:37',\n",
    "#  'category': 'economy',\n",
    "#  'press': 'YTN ',\n",
    "#  'title': '추경호 중기 수출지원 총력 무역금융 40조 확대',\n",
    "#  'document': '앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.',\n",
    "#  'link': 'https://n.news.naver.com/mnews/article/052/0001759333?sid=101',\n",
    "#  'summary': '올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.'}\n",
    "def get_keywords(data): \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"뉴스에서 핵심 키워드를 예시와 같이 명사로 5개 추출해주세요\\n\\n[뉴스전문] {data['document']}\\n\\n[예시] 키워드1, 키워드2, 키워드3, 키워드4, 키워드5\"\n",
    "        }\n",
    "    ]\n",
    "    output = client.chat.completions.create(model=\"gpt-5-nano\", messages=messages)\n",
    "    output = output.choices[0].message.content\n",
    "\n",
    "    data[\"keywords\"] = output\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2d48d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '2022-07-03 17:14:37',\n",
       " 'category': 'economy',\n",
       " 'press': 'YTN ',\n",
       " 'title': '추경호 중기 수출지원 총력 무역금융 40조 확대',\n",
       " 'document': '앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.',\n",
       " 'link': 'https://n.news.naver.com/mnews/article/052/0001759333?sid=101',\n",
       " 'summary': '올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.',\n",
       " 'text': '<bos><start_of_turn>user\\n다음 글을 요약해주세요:\\n\\n 앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.<end_of_turn>\\n<start_of_turn>model\\n올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.<end_of_turn>\\n<eos>'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f51f7ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '2022-07-03 17:14:37',\n",
       " 'category': 'economy',\n",
       " 'press': 'YTN ',\n",
       " 'title': '추경호 중기 수출지원 총력 무역금융 40조 확대',\n",
       " 'document': '앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.',\n",
       " 'link': 'https://n.news.naver.com/mnews/article/052/0001759333?sid=101',\n",
       " 'summary': '올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.',\n",
       " 'text': '<bos><start_of_turn>user\\n다음 글을 요약해주세요:\\n\\n 앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.<end_of_turn>\\n<start_of_turn>model\\n올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.<end_of_turn>\\n<eos>',\n",
       " 'keywords': '수출확대, 무역금융, 물류비, 임시선박, 무역수지'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_keywords(sample_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10728d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function get_keywords at 0x0000022BDA905B40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e228460cd7b41ec8b5707e6a8b11c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary', 'text', 'keywords'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset = sample_dataset.map(get_keywords)\n",
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b171597f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf3c7f002094794a911f35c424cb69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 만든 데이터 저장하기 \n",
    "sample_dataset.save_to_disk(\"naver_summary_custom_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e640fc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary', 'text', 'keywords'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 만든 데이터 로드 \n",
    "from datasets import load_from_disk\n",
    "sample_dataset = load_from_disk(\"naver_summary_custom_data\")\n",
    "print(sample_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae41d7b",
   "metadata": {},
   "source": [
    "### 3) 학습 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80670303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP2. document, summary, keywords를 가지고 messages 형태의 데이터를 만든다.\n",
    "def make_prompt(data):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"뉴스를 요약해주세요.\\n\\n 그리고 핵심 키워드를 예시와 같이 명사로 5개 추출해주세요\\n\\n[뉴스전문] {data['document']}\\n\\n[예시]키워드: 키워드1, 키워드2, 키워드3, 키워드4, 키워드5\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"[키워드]: {data['keywords']}\\n\\n[뉴스요약]: {data['summary']}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=False\n",
    "        ) + \"<eos>\" # <eos>는 따로 추가해줘야 함\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93b4f7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '2022-07-03 17:14:37',\n",
       " 'category': 'economy',\n",
       " 'press': 'YTN ',\n",
       " 'title': '추경호 중기 수출지원 총력 무역금융 40조 확대',\n",
       " 'document': '앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.',\n",
       " 'link': 'https://n.news.naver.com/mnews/article/052/0001759333?sid=101',\n",
       " 'summary': '올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.',\n",
       " 'text': '<bos><start_of_turn>user\\n다음 글을 요약해주세요:\\n\\n 앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.<end_of_turn>\\n<start_of_turn>model\\n올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.<end_of_turn>\\n<eos>',\n",
       " 'keywords': '수출, 무역수지, 물류비, 무역금융, 임시선박'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00a4d5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<bos><start_of_turn>user\\n뉴스를 요약해주세요.\\n\\n 그리고 핵심 키워드를 예시와 같이 명사로 5개 추출해주세요\\n\\n[뉴스전문] 앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.\\n\\n[예시]키워드: 키워드1, 키워드2, 키워드3, 키워드4, 키워드5<end_of_turn>\\n<start_of_turn>model\\n[키워드]: 수출, 무역수지, 물류비, 무역금융, 임시선박\\n\\n[뉴스요약]: 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.<end_of_turn>\\n<eos>'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prompt(sample_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83531efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbd86777d9e43c2a76b5f0f83484cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary', 'text', 'keywords'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset = sample_dataset.map(make_prompt)\n",
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcce501b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\n뉴스를 요약해주세요.\\n\\n 그리고 핵심 키워드를 예시와 같이 명사로 5개 추출해주세요\\n\\n[뉴스전문] 앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.\\n\\n[예시]키워드: 키워드1, 키워드2, 키워드3, 키워드4, 키워드5<end_of_turn>\\n<start_of_turn>model\\n[키워드]: 수출, 무역수지, 물류비, 무역금융, 임시선박\\n\\n[뉴스요약]: 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.<end_of_turn>\\n<eos>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2355380",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = sample_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_set = split_dataset[\"train\"]\n",
    "val_set = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f6e0fc",
   "metadata": {},
   "source": [
    "## 2. 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4c7b7",
   "metadata": {},
   "source": [
    "### 1) 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f465dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 가중치를 업데이트 하는 방식 : Full FT\n",
    "# 가중치 일부만 바꾸는 방식: PEFT(Parameter-Efficient FT) \n",
    "## LoRA(저차원행렬의 곱셈) \"기존의 가중치는 고정하고 Adapter의 가중치를 업데이트한다.\"\n",
    "## uv add peft\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    # r: 저차원 랭크(rank). 높일수록 표현력↑, VRAM/연산량↑\n",
    "    #  - 소형 모델/짧은 데이터: 4~8\n",
    "    #  - 중형 이상/복잡 과제  : 8~16 (필요시 32)\n",
    "    r=6,\n",
    "\n",
    "    # lora_alpha: LoRA 스케일(α). 최종 적용은 (α / r) * (B @ A)\n",
    "    #  - 너무 크면 과적합/불안정, 너무 작으면 표현력↓\n",
    "    #  - 보통 r의 2~4배 근처(예: r=8 → α=16~32)\n",
    "    lora_alpha=8,\n",
    "\n",
    "    # lora_dropout: 어댑터 경로에만 적용되는 드롭아웃 확률\n",
    "    #  - 과적합 방지. 데이터 적거나 문체 과적응 막고 싶을 때 0.05~0.1\n",
    "    #  - 매우 작은 데이터(수십~수백)면 0.0~0.05도 사용\n",
    "    lora_dropout=0.05,\n",
    "\n",
    "    # target_modules: LoRA를 적용할 서브모듈 이름 목록\n",
    "    #  - 주로 Attention의 프로젝션 레이어에 적용\n",
    "    #  - LLaMA/Gemma 계열: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]가 기본\n",
    "    #  - 추가로 FFN에 확장하려면 [\"up_proj\",\"down_proj\",\"gate_proj\"]도 고려\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "\n",
    "    # bias: 바이어스 파라미터 업데이트 방식\n",
    "    #  - \"none\"   : 바이어스는 고정(일반적)\n",
    "    #  - \"lora_only\": LoRA에 해당하는 바이어스만 학습\n",
    "    #  - \"all\"    : 모든 바이어스 학습(권장 X, 원본 오염 위험)\n",
    "    bias=\"none\",\n",
    "\n",
    "    # task_type: 태스크 유형. Causal LM(자연어 생성)에는 \"CAUSAL_LM\"\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "\n",
    "    # modules_to_save: LoRA 외에 함께 학습/저장할 모듈 이름\n",
    "    #  - 예: \"lm_head\" 등을 LoRA와 함께 미세 조정하고 싶을 때\n",
    "    # modules_to_save=[\"lm_head\"],\n",
    "\n",
    "    # init_lora_weights: LoRA 가중치 초기화 방법\n",
    "    #  - True(기본): 권장 초기화로 안정적 시작\n",
    "    #  - False     : 수동 초기화 시\n",
    "    # init_lora_weights=True,\n",
    "\n",
    "    # use_rslora: Rank-Stabilized LoRA (표현력 향상 목적)\n",
    "    #  - True 시 랭크에 안정화 스케일링 적용(큰 r에서 수렴 안정↑)\n",
    "    # use_rslora=False,\n",
    "\n",
    "    # inference_mode: 추론 전용 모드로 생성 시 True(학습 시 False)\n",
    "    # inference_mode=False,\n",
    "\n",
    "    # layers_to_transform / layers_pattern:\n",
    "    #  - 특정 레이어 인덱스/패턴에만 LoRA 적용하고 싶을 때 사용\n",
    "    #  - 예: 마지막 N개 블록만 적용 등\n",
    "    # layers_to_transform=None,\n",
    "    # layers_pattern=None,\n",
    "\n",
    "    # lora_dtype: LoRA 어댑터의 계산 dtype 지정\n",
    "    #  - None이면 모델 dtype 따름(bfloat16/float16 권장)\n",
    "    # lora_dtype=None,\n",
    "\n",
    "    # rank_pattern / alpha_pattern:\n",
    "    #  - 모듈별로 서로 다른 r/alpha를 주고 싶을 때 딕셔너리로 지정\n",
    "    #  - 예: {\"q_proj\": 16, \"v_proj\": 8} 등 세밀 조정\n",
    "    # rank_pattern=None,\n",
    "    # alpha_pattern=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51252a2e",
   "metadata": {},
   "source": [
    "### 2) 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb25fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output\",           # 체크포인트/로그 저장 위치 (필수)\n",
    "\n",
    "    # 학습 길이\n",
    "    num_train_epochs=1,              # 에폭 수 (기본 3.0). max_steps가 양수면 이 값은 무시됨.\n",
    "\n",
    "    # 배치/메모리\n",
    "    per_device_train_batch_size=2,   # 장치당 배치(기본 8). OOM 나면 1로 낮춤\n",
    "    gradient_accumulation_steps=8,   # 유효배치=2*8=16\n",
    "    bf16=True,                       # 4070 권장\n",
    "    gradient_checkpointing=False,    # 메모리 절약(약간 느려짐)\n",
    "\n",
    "    # 최적화\n",
    "    learning_rate=2e-4,              # 기본은 5e-5. SFT/LoRA면 조금 높게도 사용\n",
    "    lr_scheduler_type=\"cosine\",      # 기본은 \"linear\"\n",
    "    warmup_ratio=0.0,                # 기본 0.0 → 안정화를 위해 소량 워밍업 권장\n",
    "\n",
    "    # 평가/저장 (신규 키: eval_strategy)\n",
    "    # eval_strategy=\"epoch\",           # epoch/steps/no\n",
    "    save_strategy=\"epoch\",           # epoch/steps/no/best\n",
    "    save_total_limit=2,              # 오래된 체크포인트 자동 정리\n",
    "\n",
    "    # 로깅\n",
    "    logging_strategy=\"steps\",        # 기본 \"steps\"\n",
    "    logging_steps=1,                 # 기본 500 → 실습에선 더 자주 보이게\n",
    "    report_to=\"none\",                # W&B/TensorBoard 안 쓸 때\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb2ac814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b77e0c78974cc58c8943fc91e84c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbc85d5ca0d48918870f2a0449a7ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1ddf42232a4649a285ef6cf9158171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a380aefe7447a9b964e06cd4a21530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8eb12e9876f4f67899d6c57009fc234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252e626975a446c9aab132878fc2e7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# TRL의 SFTTrainer: \"지시문/대화 데이터\"를 가장 간단히 학습하는 Trainer\n",
    "#  - collator를 따로 만들 필요가 없음(내부에서 처리)\n",
    "#  - peft_config를 넘기면 LoRA/QLoRA 어댑터만 학습하도록 자동 구성\n",
    "#  - dataset에 text 열이 있으면 dataset_text_field=\"text\"로 지정\n",
    "# ------------------------------------------------------------\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                  # AutoModelForCausalLM 등\n",
    "    args=args,                    # 학습/로깅/저장은 TrainingArguments에서 통제\n",
    "    train_dataset=sample_dataset, # Dataset | torch Dataset\n",
    "    eval_dataset=val_set,       # 선택. 있으면 eval 전략에 따라 평가\n",
    "    peft_config=lora_config,      # LoRA/QLoRA 적용. (없으면 Full FT)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0b64b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 GPU 메모리 캐시 비우기\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 학습 시작\n",
    "# trainer.train() 노트북(+gpu) 하드웨어로는 불가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1164479",
   "metadata": {},
   "source": [
    "### 3) 학습된 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0761314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어댑터 모델 저장\n",
    "adapter_name = \"lora_adapter\"\n",
    "\n",
    "trainer.model.save_pretrained(adapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d7ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 모델과 adaptor 모델을 merge하는 것이 필요\n",
    "from peft import PeftModel\n",
    "save_model_name = \"gemma-3-1b-sum-ko\"\n",
    "\n",
    "# 기존 모델\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map='auto', \n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# adaptor 모델\n",
    "model = PeftModel.from_pretrained(\n",
    "    model, \n",
    "    adapter_name, \n",
    "    device_map='auto', \n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(save_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb23af",
   "metadata": {},
   "source": [
    "## 3. 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d131a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model = AutoModelForCausalLM.from_pretrained(save_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77025a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe_finetuned = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=finetune_model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a436f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = dataset['test']['document'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528aec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"뉴스를 요약해주세요.\\n\\n 그리고 핵심 키워드를 예시와 같이 명사로 5개 추출해주세요\\n\\n[뉴스전문] {doc}\\n\\n[예시]키워드: 키워드1, 키워드2, 키워드3, 키워드4, 키워드5\"\n",
    "    }\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3dbb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pipe_finetuned(\n",
    "    prompt,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
