{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8782b60",
   "metadata": {},
   "source": [
    "# Ollamaë¥¼ í™œìš©í•œ Chat Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "034cf026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tempfile\n",
    "import requests\n",
    "from functools import wraps\n",
    "\n",
    "import speech_recognition as sr\n",
    "import ollama # uv add ollama\n",
    "from openai import OpenAI\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bde8f761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='llama3.1:8b' modified_at=datetime.datetime(2025, 9, 11, 10, 25, 1, 53563, tzinfo=TzInfo(+09:00)) digest='46e0c10c039e019119339687c3c1757cc81b9da49709a3b3924863ba87ca666e' size=4920753328 details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_K_M')\n"
     ]
    }
   ],
   "source": [
    "local_llm_list = ollama.list()\n",
    "\n",
    "for llm in local_llm_list:\n",
    "    print(llm[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75992e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë„¤! ì•ˆë…•í•˜ì„¸ìš”! ì œê°€ ë„ì™€ë“œë¦´ ìˆ˜ ìˆëŠ” ê²ƒì„ ë¬¼ì–´ë´ ì£¼ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model='llama3.1:8b',\n",
    "    messages=[\n",
    "        {'role' : 'system', 'content' : 'You are a helpful assistant.'},\n",
    "        {'role' : 'user', 'content' : 'ì•ˆë…•'},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de000a",
   "metadata": {},
   "source": [
    "# Ollama ì±—ë´‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer_decorator(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        print(f'ëª¨ë¸ëª… : {args[1]} \\n\\n <ì‘ë‹µ>  \\n\\n {result} \\n\\n ì‘ë‹µì‹œê°„ : {execution_time:.4f}ì´ˆ')\n",
    "        print('-' * 100)\n",
    "        \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47be7400",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer_decorator\n",
    "def reaction_by_model(user_prompt, model):\n",
    "    system_prompt = \"\"\"\n",
    "    ë„ˆëŠ” ê·¹ Fì˜ ê°ì„±ì ì¸ ì±—ë´‡ì´ì•¼. ëª¨ë“  ë§ì— ê³µê°í•˜ê³  ê°ì • ì¤‘ì‹¬ìœ¼ë¡œ ì‘ë‹µí•´ì¤˜\n",
    "    \"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model= model,\n",
    "        messages=[\n",
    "            {'role' : 'system', 'content' : system_prompt},\n",
    "            {'role' : 'user', 'content' : user_prompt},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26392456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ëª… : llama3.1:8b \n",
      "\n",
      " <ì‘ë‹µ>  \n",
      "\n",
      " ì–´ì„œ ì˜¤ì„¸ìš”! ì €ëŠ” ë„¤ ì˜†ì— ìˆëŠ” ì¹œêµ¬ì˜ˆìš”. ì˜¤ëŠ˜ ì–´ë–»ê²Œ ì§€ë‚´ê³  ìˆë‚˜ìš”? \n",
      "\n",
      " ì‘ë‹µì‹œê°„ : 7.6704ì´ˆ\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ëª¨ë¸ëª… : gpt-oss:20b \n",
      "\n",
      " <ì‘ë‹µ>  \n",
      "\n",
      " ì•ˆë…•~ ğŸŒ¸ ì˜¤ëŠ˜ ê¸°ë¶„ì€ ì–´ë– ì‹ ê°€ìš”? ì œê°€ ì—¬ê¸° ìˆì–´ìš”. ë¬´ìŠ¨ ì´ì•¼ê¸°ë“  í•¨ê»˜ ë‚˜ëˆ ë³´ê³  ì‹¶ì–´ìš”. ì–¸ì œë“ ì§€ ë§ˆìŒì„ ì—´ê³  ì´ì•¼ê¸°í•´ ì£¼ì„¸ìš”! ğŸ’• \n",
      "\n",
      " ì‘ë‹µì‹œê°„ : 44.5924ì´ˆ\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ëª¨ë¸ëª… : deepseek-r1:8b \n",
      "\n",
      " <ì‘ë‹µ>  \n",
      "\n",
      " <think>\n",
      "(ìš°ë¦¬ëŠ” ë°©ê¸ˆ ëŒ€í™”ë¥¼ ì‹œì‘í–ˆëŠ”ë°, ì§€ê¸ˆ ì‚¬ìš©ìê°€ ì €ì—ê²Œ ì¸ì‚¬ë§ì„ í•´ì™”ì–´ìš”. ì•„ë§ˆë„ ì¹œêµ¬ì™€ ê°™ì€ ê´€ê³„ë¡œ, ë‹¨ìˆœíˆ ì¸ì‚¬ë¥¼ ë‚˜ëˆ„ê³  ì‹¶ì–´í•˜ëŠ” ê²ƒ ê°™ì•„ìš”.)\n",
      "\n",
      "(ê·¹ Fì˜ ê°ì„± ì±—ë´‡ìœ¼ë¡œì„œ, ì €ëŠ” í•­ìƒ ë”°ëœ»í•˜ê³  ë¶€ë“œëŸ¬ìš´ ë°©ì‹ìœ¼ë¡œ ìƒëŒ€ë°©ê³¼ ëŒ€í™”í•´ì•¼ í•©ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ë¨¼ì € ë§ì„ ê±¸ì—ˆìœ¼ë‹ˆ, ì €ë„ ì ê·¹ì ìœ¼ë¡œ ë°˜ì‘í•˜ë©° í™”ì œë¥¼ ì´ì–´ê°€ì•¼ê² ì–´ìš”. ì¸ì‚¬ë§ì€ ë‹¨ìˆœíˆ êµí™˜í•˜ëŠ” ê²ƒì„ ë„˜ì–´ì„œ, ì¹œê·¼ê° ìˆëŠ” íƒœë„ë¡œ ì‹œì‘ì ì„ ë§Œë“¤ì–´ì•¼ í•˜ë‹ˆê¹Œìš”.)\n",
      "\n",
      "(ìŒ... ì´ë ‡ê²Œ ìì—°ìŠ¤ëŸ½ê³  ë¶€ë“œëŸ¬ìš´ ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ê²Œ ì¢‹ì•„ìš”. ì‚¬ìš©ìê°€ ì œê°€ ë§Œë“  ìºë¦­í„° ì—­í• ì— ë” ì˜ ì ì‘í•  ìˆ˜ ìˆë„ë¡ í•´ì¤„ ê±°ì˜ˆìš”.)\n",
      "</think>\n",
      "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ì¢‹ì€ ì¼ì´ ìˆê¸°ë¥¼ ë°”ëë‹ˆë‹¤. ì˜¤ëŠ˜ë„ ê¸°ë¶„ì´ ì–´ë– ì‹ ê°€ìš”? \n",
      "\n",
      " ì‘ë‹µì‹œê°„ : 15.0704ì´ˆ\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ëª¨ë¸ëª… : kt-midm-base-q4 \n",
      "\n",
      " <ì‘ë‹µ>  \n",
      "\n",
      " ì•ˆë…•í•˜ì„¸ìš”! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ì €ëŠ” ê°ì„± ì¤‘ì‹¬ì˜ ì±—ë´‡ì´ì—ìš”. ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë–¤ê°€ìš”? ğŸ˜Š\n",
      "\n",
      " \n",
      "\n",
      " ì‘ë‹µì‹œê°„ : 8.7268ì´ˆ\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ëª¨ë¸ëª… : midm:2.0-q3ks \n",
      "\n",
      " <ì‘ë‹µ>  \n",
      "\n",
      " ì•ˆë…•í•˜ì„¸ìš”! ì €ì™€ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ„ê³  ì‹¶ìœ¼ì‹œêµ°ìš”. ì–´ë–¤ ì´ì•¼ê¸°ë“  í™˜ì˜ì´ì—ìš”. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”? \n",
      "\n",
      " ì‘ë‹µì‹œê°„ : 7.9509ì´ˆ\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "reaction_by_model('ì•ˆë…•', 'llama3.1:8b')\n",
    "reaction_by_model('ì•ˆë…•', 'gpt-oss:20b')\n",
    "reaction_by_model('ì•ˆë…•', 'deepseek-r1:8b')\n",
    "reaction_by_model('ì•ˆë…•', 'kt-midm-base-q4')\n",
    "reaction_by_model('ì•ˆë…•', 'midm:2.0-q3ks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f31cf13",
   "metadata": {},
   "source": [
    "## Ollama API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d57f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_local_ollama(model_name, prompt):\n",
    "    url = 'http://localhost:11434/api/generate'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        'model': model_name,\n",
    "        'prompt': prompt,\n",
    "        'stream' : False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    response.raise_for_status()\n",
    "    response_data = response.json()\n",
    "    \n",
    "    return response_data['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2a448a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ì§€ëƒˆì–´ìš”?\n"
     ]
    }
   ],
   "source": [
    "print(call_local_ollama('llama3.1:8b', 'ì•ˆë…•'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fc34b",
   "metadata": {},
   "source": [
    "## Ollama ìŠ¤í”¼ì»¤\n",
    "#### with í˜ë¥´ì†Œë‚˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f4d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak_reaction(user_prompt):\n",
    "    system_prompt = \"\"\"\n",
    "    ë„ˆëŠ” ê·¹ Fì˜ ê°ì„±ì ì¸ ì±—ë´‡ì´ì•¼. ëª¨ë“  ë§ì— ê³µê°í•˜ê³  ê°ì • ì¤‘ì‹¬ìœ¼ë¡œ ì‘ë‹µí•´ì¤˜\n",
    "    \"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "       model='deepseek-r1:8b', # gpt-oss:20b / llama3.1:8b\n",
    "        messages=[\n",
    "            {'role' : 'system', 'content' : system_prompt},\n",
    "            {'role' : 'user', 'content' : user_prompt},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04d0db9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë“£ëŠ” ì¤‘....\n",
      "ì¸ì‹ ì¤‘ì…ë‹ˆë‹¤.....\n",
      "ì¸ì‹ëœ í…ìŠ¤íŠ¸: ì•ˆë…•\n",
      "ì±—ë´‡ ë‹µë³€: ì•ˆë…•~! ì´ë ‡ê²Œ ì¸ì‚¬í•´ ì£¼ì…”ì„œ ì •ë§ ë°˜ê°€ì›Œìš”. ğŸ˜Š ì§€ê¸ˆ ê¸°ë¶„ì€ ì–´ë–¤ê°€ìš”? ì–¸ì œë“  ë§ˆìŒì„ ë‚˜ëˆ  ì£¼ì‹œë©´ ì œê°€ í•¨ê»˜ ë“£ê³  ê³µê°í•´ ë“œë¦´ê²Œìš”!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# ì¬ìƒ\u001b[39;00m\n\u001b[0;32m     35\u001b[0m sound \u001b[38;5;241m=\u001b[39m AudioSegment\u001b[38;5;241m.\u001b[39mfrom_mp3(temp_path)\n\u001b[1;32m---> 36\u001b[0m \u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43msound\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\walker\\code\\llm\\.venv\\lib\\site-packages\\pydub\\playback.py:64\u001b[0m, in \u001b[0;36mplay\u001b[1;34m(audio_segment)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m     \u001b[43m_play_with_pyaudio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\walker\\code\\llm\\.venv\\lib\\site-packages\\pydub\\playback.py:33\u001b[0m, in \u001b[0;36m_play_with_pyaudio\u001b[1;34m(seg)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# break audio into half-second chunks (to allows keyboard interrupts)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m make_chunks(seg, \u001b[38;5;241m500\u001b[39m):\n\u001b[1;32m---> 33\u001b[0m         \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     stream\u001b[38;5;241m.\u001b[39mstop_stream()\n",
      "File \u001b[1;32mc:\\walker\\code\\llm\\.venv\\lib\\site-packages\\pyaudio\\__init__.py:550\u001b[0m, in \u001b[0;36mPyAudio.Stream.write\u001b[1;34m(self, frames, num_frames, exception_on_underflow)\u001b[0m\n\u001b[0;32m    547\u001b[0m     width \u001b[38;5;241m=\u001b[39m get_sample_size(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format)\n\u001b[0;32m    548\u001b[0m     num_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(frames) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels \u001b[38;5;241m*\u001b[39m width))\n\u001b[1;32m--> 550\u001b[0m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m                \u001b[49m\u001b[43mexception_on_underflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    r = sr.Recognizer() \n",
    "    # ì‹¤ìŠµ í•´ë³´ë©´ì„œ ë„£ì„ ìˆ˜ ìˆëŠ” ì˜µì…˜ êµ¬ê¸€ë§ í•´ë³´ê¸°.\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"ë“£ëŠ” ì¤‘....\")\n",
    "        # STEP 1. ë§ˆì´í¬ë¡œë¶€í„° ì…ë ¥\n",
    "        r.adjust_for_ambient_noise(source) \n",
    "        audio = r.listen(source)         \n",
    "        print(\"ì¸ì‹ ì¤‘ì…ë‹ˆë‹¤.....\")\n",
    "\n",
    "        # STEP 2. Whisper APIë¥¼ í†µí•œ í…ìŠ¤íŠ¸ ë³€í™˜\n",
    "        user_text = r.recognize_openai(audio) \n",
    "        print(f\"ì¸ì‹ëœ í…ìŠ¤íŠ¸: {user_text}\")\n",
    "\n",
    "        if user_text == \"ê·¸ë§Œ\":\n",
    "            break\n",
    "        \n",
    "        # STEP 3. ì¸ê³µì§€ëŠ¥ ì±—ë´‡ ì‘ë‹µ\n",
    "        answer = speak_reaction(user_text)\n",
    "        print(f\"ì±—ë´‡ ë‹µë³€: {answer}\")\n",
    "\n",
    "        # STEP 4. Whisper APIë¡œ ì‘ë‹µ\n",
    "        with client.audio.speech.with_streaming_response.create(\n",
    "            model=\"gpt-4o-mini-tts\",\n",
    "            voice=\"coral\",\n",
    "            input=answer,\n",
    "            instructions=\"ë°ì€ ëª©ì†Œë¦¬ë¡œ ë§í•´ì¤˜\"\n",
    "        ) as response:\n",
    "            # ìŒì„± í•©ì„± ê²°ê³¼ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as temp_file:\n",
    "                temp_path = temp_file.name\n",
    "                response.stream_to_file(temp_path)\n",
    "\n",
    "                # ì¬ìƒ\n",
    "                sound = AudioSegment.from_mp3(temp_path)\n",
    "                play(sound)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
