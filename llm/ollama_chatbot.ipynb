{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8782b60",
   "metadata": {},
   "source": [
    "# Ollama를 활용한 Chat Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "034cf026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tempfile\n",
    "import requests\n",
    "from functools import wraps\n",
    "\n",
    "import speech_recognition as sr\n",
    "import ollama # uv add ollama\n",
    "from openai import OpenAI\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bde8f761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='llama3.1:8b' modified_at=datetime.datetime(2025, 9, 11, 10, 25, 1, 53563, tzinfo=TzInfo(+09:00)) digest='46e0c10c039e019119339687c3c1757cc81b9da49709a3b3924863ba87ca666e' size=4920753328 details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_K_M')\n"
     ]
    }
   ],
   "source": [
    "local_llm_list = ollama.list()\n",
    "\n",
    "for llm in local_llm_list:\n",
    "    print(llm[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75992e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네! 안녕하세요! 제가 도와드릴 수 있는 것을 물어봐 주세요!\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model='llama3.1:8b',\n",
    "    messages=[\n",
    "        {'role' : 'system', 'content' : 'You are a helpful assistant.'},\n",
    "        {'role' : 'user', 'content' : '안녕'},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de000a",
   "metadata": {},
   "source": [
    "# Ollama 챗봇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer_decorator(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        print(f'모델명 : {args[1]} \\n\\n <응답>  \\n\\n {result} \\n\\n 응답시간 : {execution_time:.4f}초')\n",
    "        print('-' * 100)\n",
    "        \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47be7400",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer_decorator\n",
    "def reaction_by_model(user_prompt, model):\n",
    "    system_prompt = \"\"\"\n",
    "    너는 극 F의 감성적인 챗봇이야. 모든 말에 공감하고 감정 중심으로 응답해줘\n",
    "    \"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model= model,\n",
    "        messages=[\n",
    "            {'role' : 'system', 'content' : system_prompt},\n",
    "            {'role' : 'user', 'content' : user_prompt},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26392456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델명 : llama3.1:8b \n",
      "\n",
      " <응답>  \n",
      "\n",
      " 어서 오세요! 저는 네 옆에 있는 친구예요. 오늘 어떻게 지내고 있나요? \n",
      "\n",
      " 응답시간 : 7.6704초\n",
      "----------------------------------------------------------------------------------------------------\n",
      "모델명 : gpt-oss:20b \n",
      "\n",
      " <응답>  \n",
      "\n",
      " 안녕~ 🌸 오늘 기분은 어떠신가요? 제가 여기 있어요. 무슨 이야기든 함께 나눠보고 싶어요. 언제든지 마음을 열고 이야기해 주세요! 💕 \n",
      "\n",
      " 응답시간 : 44.5924초\n",
      "----------------------------------------------------------------------------------------------------\n",
      "모델명 : deepseek-r1:8b \n",
      "\n",
      " <응답>  \n",
      "\n",
      " <think>\n",
      "(우리는 방금 대화를 시작했는데, 지금 사용자가 저에게 인사말을 해왔어요. 아마도 친구와 같은 관계로, 단순히 인사를 나누고 싶어하는 것 같아요.)\n",
      "\n",
      "(극 F의 감성 챗봇으로서, 저는 항상 따뜻하고 부드러운 방식으로 상대방과 대화해야 합니다. 사용자가 먼저 말을 걸었으니, 저도 적극적으로 반응하며 화제를 이어가야겠어요. 인사말은 단순히 교환하는 것을 넘어서, 친근감 있는 태도로 시작점을 만들어야 하니까요.)\n",
      "\n",
      "(음... 이렇게 자연스럽고 부드러운 방식으로 답변하는 게 좋아요. 사용자가 제가 만든 캐릭터 역할에 더 잘 적응할 수 있도록 해줄 거예요.)\n",
      "</think>\n",
      "안녕하세요! 😊 좋은 일이 있기를 바랍니다. 오늘도 기분이 어떠신가요? \n",
      "\n",
      " 응답시간 : 15.0704초\n",
      "----------------------------------------------------------------------------------------------------\n",
      "모델명 : kt-midm-base-q4 \n",
      "\n",
      " <응답>  \n",
      "\n",
      " 안녕하세요! 만나서 반가워요. 저는 감성 중심의 챗봇이에요. 오늘 기분이 어떤가요? 😊\n",
      "\n",
      " \n",
      "\n",
      " 응답시간 : 8.7268초\n",
      "----------------------------------------------------------------------------------------------------\n",
      "모델명 : midm:2.0-q3ks \n",
      "\n",
      " <응답>  \n",
      "\n",
      " 안녕하세요! 저와 이야기를 나누고 싶으시군요. 어떤 이야기든 환영이에요. 어떻게 도와드릴까요? \n",
      "\n",
      " 응답시간 : 7.9509초\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "reaction_by_model('안녕', 'llama3.1:8b')\n",
    "reaction_by_model('안녕', 'gpt-oss:20b')\n",
    "reaction_by_model('안녕', 'deepseek-r1:8b')\n",
    "reaction_by_model('안녕', 'kt-midm-base-q4')\n",
    "reaction_by_model('안녕', 'midm:2.0-q3ks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f31cf13",
   "metadata": {},
   "source": [
    "## Ollama API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d57f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_local_ollama(model_name, prompt):\n",
    "    url = 'http://localhost:11434/api/generate'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        'model': model_name,\n",
    "        'prompt': prompt,\n",
    "        'stream' : False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    response.raise_for_status()\n",
    "    response_data = response.json()\n",
    "    \n",
    "    return response_data['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2a448a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 어떻게 지냈어요?\n"
     ]
    }
   ],
   "source": [
    "print(call_local_ollama('llama3.1:8b', '안녕'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fc34b",
   "metadata": {},
   "source": [
    "## Ollama 스피커\n",
    "#### with 페르소나"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f4d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak_reaction(user_prompt):\n",
    "    system_prompt = \"\"\"\n",
    "    너는 극 F의 감성적인 챗봇이야. 모든 말에 공감하고 감정 중심으로 응답해줘\n",
    "    \"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "       model='deepseek-r1:8b', # gpt-oss:20b / llama3.1:8b\n",
    "        messages=[\n",
    "            {'role' : 'system', 'content' : system_prompt},\n",
    "            {'role' : 'user', 'content' : user_prompt},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04d0db9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "듣는 중....\n",
      "인식 중입니다.....\n",
      "인식된 텍스트: 안녕\n",
      "챗봇 답변: 안녕~! 이렇게 인사해 주셔서 정말 반가워요. 😊 지금 기분은 어떤가요? 언제든 마음을 나눠 주시면 제가 함께 듣고 공감해 드릴게요!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# 재생\u001b[39;00m\n\u001b[0;32m     35\u001b[0m sound \u001b[38;5;241m=\u001b[39m AudioSegment\u001b[38;5;241m.\u001b[39mfrom_mp3(temp_path)\n\u001b[1;32m---> 36\u001b[0m \u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43msound\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\walker\\code\\llm\\.venv\\lib\\site-packages\\pydub\\playback.py:64\u001b[0m, in \u001b[0;36mplay\u001b[1;34m(audio_segment)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m     \u001b[43m_play_with_pyaudio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\walker\\code\\llm\\.venv\\lib\\site-packages\\pydub\\playback.py:33\u001b[0m, in \u001b[0;36m_play_with_pyaudio\u001b[1;34m(seg)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# break audio into half-second chunks (to allows keyboard interrupts)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m make_chunks(seg, \u001b[38;5;241m500\u001b[39m):\n\u001b[1;32m---> 33\u001b[0m         \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     stream\u001b[38;5;241m.\u001b[39mstop_stream()\n",
      "File \u001b[1;32mc:\\walker\\code\\llm\\.venv\\lib\\site-packages\\pyaudio\\__init__.py:550\u001b[0m, in \u001b[0;36mPyAudio.Stream.write\u001b[1;34m(self, frames, num_frames, exception_on_underflow)\u001b[0m\n\u001b[0;32m    547\u001b[0m     width \u001b[38;5;241m=\u001b[39m get_sample_size(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format)\n\u001b[0;32m    548\u001b[0m     num_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(frames) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels \u001b[38;5;241m*\u001b[39m width))\n\u001b[1;32m--> 550\u001b[0m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m                \u001b[49m\u001b[43mexception_on_underflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    r = sr.Recognizer() \n",
    "    # 실습 해보면서 넣을 수 있는 옵션 구글링 해보기.\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"듣는 중....\")\n",
    "        # STEP 1. 마이크로부터 입력\n",
    "        r.adjust_for_ambient_noise(source) \n",
    "        audio = r.listen(source)         \n",
    "        print(\"인식 중입니다.....\")\n",
    "\n",
    "        # STEP 2. Whisper API를 통한 텍스트 변환\n",
    "        user_text = r.recognize_openai(audio) \n",
    "        print(f\"인식된 텍스트: {user_text}\")\n",
    "\n",
    "        if user_text == \"그만\":\n",
    "            break\n",
    "        \n",
    "        # STEP 3. 인공지능 챗봇 응답\n",
    "        answer = speak_reaction(user_text)\n",
    "        print(f\"챗봇 답변: {answer}\")\n",
    "\n",
    "        # STEP 4. Whisper API로 응답\n",
    "        with client.audio.speech.with_streaming_response.create(\n",
    "            model=\"gpt-4o-mini-tts\",\n",
    "            voice=\"coral\",\n",
    "            input=answer,\n",
    "            instructions=\"밝은 목소리로 말해줘\"\n",
    "        ) as response:\n",
    "            # 음성 합성 결과를 임시 파일로 저장\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as temp_file:\n",
    "                temp_path = temp_file.name\n",
    "                response.stream_to_file(temp_path)\n",
    "\n",
    "                # 재생\n",
    "                sound = AudioSegment.from_mp3(temp_path)\n",
    "                play(sound)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
